---
title: "Aula 1"
author: "Manoel Galdino"
date: "2023-05-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Fundamentals of Regreession

https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/

Nós definidmos o erro de um estimador ou previsão de uma v.a. $Y$, $m$, como $e = m - Y$.

Ali, estávamos interessados em ver quanto um estimador errava.
Agora, estamos interesados em ver quanto uma previsão é errada por um estimador $m$ qualquer. Iremos, portanto, definir o erro $e$ como: $Y - m$. Obviamente o valor absoluto dessa iferença, ou o quadrado dela, darão o mesmo resultado que se eu calculasse a diferença com sinal trocado, $m - Y$. Então, é mais uma questão de conveniência.

Qual o valor de m que minimiza $\mathbb{E}[(Y - m)^2]$?

Vamos aplicar o truque matemático que usamos para mostrar que o EQM pode ser decomposto em viés ao quadrado do estimador mais variância do estimador.


1. Começamos somando e subtraindo $\mu$

$\mathbb{E}[(Y - m)^2] = \mathbb{E}[(Y - \mu + \mu - m)^2]$. Aqui eu somei e adicionei o mesmo valor, o que não altera a equação.

Reordenando os termos, temos:

Veja que se eu chamar  $Y - \mu = a$ e $\mu - m = b$, tenho:

$\mathbb{E}[(a + b)^2]$. Aplicando a regra do quadrado:

$\mathbb{E}[a^2 + 2*a*b + b^2]$

Substituindo $a$ e $b$, temos:
$\mathbb{E}[(Y - \mu )^2 + 2*(Y - \mu )*(\mu - m) + (\mu - m)^2]$

Lineraridade:

$\mathbb{E}[(Y - \mu )^2] + 2*(\mu - m)*\mathbb{E}[(Y - \mu )] + \mathbb{E}[(\mu - m)^2]$

todo o termo multiplicado por $2$ vai para zero. De forma que obtemos:


$\mathbb{E}[(Y - \mu )^2]  + \mathbb{E}[(\mu - m)^2]$

Note que $\mathbb{E}[(Y - \mu)^2] = Var(Y)$ e $\mathbb{E}[(\mu - m)^2] > 0$ se $\mu \neq m$.

Isso significa que, sempre que $m$ for diferente da média populacional de $Y$, poderemos reduzir o erro quadrático médio da previsão adotando a média como estimador. O que é um outro jeito de dizer que a média me dá o menor erro quadrático médio.






